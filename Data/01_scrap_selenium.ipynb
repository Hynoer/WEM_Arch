{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap webtoons comments using selenium/bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from urllib.parse import urlparse, parse_qs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_comments(driver, episode_url, episode_name, episode_likes, episode_date):\n",
    "    \n",
    "    # Find all reply buttons\n",
    "    reply_buttons = driver.find_elements(By.CSS_SELECTOR, \"a.u_cbox_btn_reply\")\n",
    "\n",
    "    for button in reply_buttons:\n",
    "        try:\n",
    "            reply_count_text = button.find_element(By.CSS_SELECTOR, \"span.u_cbox_reply_cnt\").text\n",
    "            reply_count = int(reply_count_text) if reply_count_text else 0\n",
    "        except Exception:\n",
    "            reply_count = 0\n",
    "\n",
    "        if reply_count > 0:\n",
    "            driver.execute_script(\"arguments[0].click();\", button)  # Click the button\n",
    "            time.sleep(3)  # Allow time for the comments to load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    comments = soup.select('.u_cbox_comment')\n",
    "\n",
    "    comment_data = []\n",
    "\n",
    "    for comment in comments:\n",
    "        data_info = comment['data-info']\n",
    "        data_info = data_info.replace(\"'\", \"\").replace(\" \", \"\")\n",
    "        data_info_dict = dict(item.split(\":\") for item in data_info.split(\",\") if \":\" in item)\n",
    "\n",
    "        comment_id = data_info_dict.get('commentNo')\n",
    "        reply_level = data_info_dict.get('replyLevel')\n",
    "        parent_comment_no = data_info_dict.get('parentCommentNo')\n",
    "        comment_text = comment.select_one('.u_cbox_contents').text\n",
    "        comment_date = comment.select_one('.u_cbox_date')['data-value']\n",
    "        comment_author = comment.select_one('.u_cbox_nick').text\n",
    "\n",
    "        # Extract likes and dislikes\n",
    "        likes = comment.select_one('.u_cbox_tool .u_cbox_cnt_recomm').text if comment.select_one('.u_cbox_tool .u_cbox_cnt_recomm') else \"0\"\n",
    "        dislikes = comment.select_one('.u_cbox_tool .u_cbox_cnt_unrecomm').text if comment.select_one('.u_cbox_tool .u_cbox_cnt_unrecomm') else \"0\"\n",
    "\n",
    "        comment_data.append([episode_url, episode_name, episode_likes, episode_date,\\\n",
    "                             comment_id, reply_level, parent_comment_no, comment_text, comment_date, comment_author, likes, dislikes])\n",
    "    \n",
    "    # Find all reply buttons again after scraping\n",
    "    reply_buttons_after_scraping = driver.find_elements(By.CSS_SELECTOR, \"a.u_cbox_btn_reply\")\n",
    "\n",
    "    # Click the reply buttons again to close the expanded replies\n",
    "    for button in reply_buttons_after_scraping:\n",
    "        try:\n",
    "            reply_count_text = button.find_element(By.CSS_SELECTOR, \"span.u_cbox_reply_cnt\").text\n",
    "            reply_count = int(reply_count_text) if reply_count_text else 0\n",
    "        except Exception:\n",
    "            reply_count = 0\n",
    "\n",
    "        if reply_count > 0:\n",
    "            driver.execute_script(\"arguments[0].click();\", button)  # Click the button\n",
    "            time.sleep(3)  # Allow time for the comments to load\n",
    "    \n",
    "    return comment_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all comments from all episodes\n",
    "def scrape_all_comments(driver, episodes):\n",
    "    def next_button_exists(driver, next_page):\n",
    "        # Check if the next button or next 10 pages button exists on the page\n",
    "        try:\n",
    "            driver.find_element(By.CSS_SELECTOR, f\"a.u_cbox_page[data-param='{next_page}']\")\n",
    "            return True\n",
    "        except NoSuchElementException:\n",
    "            try:\n",
    "                driver.find_element(By.CSS_SELECTOR, \"a.u_cbox_next\")\n",
    "                return True\n",
    "            except NoSuchElementException:\n",
    "                return False\n",
    "\n",
    "    all_comments_data = []\n",
    "\n",
    "    # Iterate through all episodes\n",
    "    for episode in episodes:\n",
    "        episode_url = episode[\"url\"]\n",
    "        episode_name = episode['name']\n",
    "        episode_likes = episode['likes']\n",
    "        episode_date = episode['date']\n",
    "        print(f\"Scraping comments from {episode_name}\")\n",
    "        driver.get(episode_url)\n",
    "        time.sleep(5)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        current_page = 1\n",
    "        # Loop through all the pages of comments for the current episode\n",
    "        while True:\n",
    "            all_comments_data.extend(scrape_comments(driver, episode_url, episode_name, episode_likes, episode_date))\n",
    "\n",
    "            # Check if the next page button exists\n",
    "            next_page = current_page + 1\n",
    "            if not next_button_exists(driver, next_page):\n",
    "                break\n",
    "\n",
    "            # Click the next page button\n",
    "            try:\n",
    "                next_button_css = f\"a.u_cbox_page[data-param='{next_page}']\"\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, next_button_css)\n",
    "            except NoSuchElementException:\n",
    "                # If the button for the next_page is not found, try clicking the next 10 pages button\n",
    "                next_button_css = \"a.u_cbox_next\"\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, next_button_css)\n",
    "\n",
    "            # Scroll the next_button into view\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "\n",
    "            # Wait for the next_button to be clickable and click it\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, next_button_css)))\n",
    "            next_button.click()\n",
    "\n",
    "            # Wait for the next page of comments to load\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Update the current_page\n",
    "            current_page = next_page\n",
    "\n",
    "    return all_comments_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract information about all episodes\n",
    "def extract_episodes_info(driver, main_url, min_page, max_page):\n",
    "\n",
    "    episodes_info = []\n",
    "\n",
    "    for page_num in range(min_page, max_page + 1):\n",
    "        if page_num == min_page:\n",
    "            page_url = f\"{main_url}&page={page_num}?lang=en\"\n",
    "        else:\n",
    "            page_url = f\"{main_url}&page={page_num}\"\n",
    "        \n",
    "        driver.get(page_url)\n",
    "\n",
    "        episode_items = driver.find_elements(By.CSS_SELECTOR, 'li._episodeItem')\n",
    "\n",
    "        for episode_item in episode_items:\n",
    "            episode_name = episode_item.find_element(By.CSS_SELECTOR, 'span.subj').text.strip()\n",
    "            episode_url = episode_item.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
    "            # episode likes\n",
    "            likes_text = episode_item.find_element(By.CSS_SELECTOR, 'span.like_area').text.strip()\n",
    "            likes_text = likes_text.replace(\",\", \"\")  # Remove commas\n",
    "            episode_likes = int(re.search(r'\\d+', likes_text).group())  # Extract the number and convert it to an integer\n",
    "            episode_date = episode_item.find_element(By.CSS_SELECTOR, 'span.date').text.strip()\n",
    "\n",
    "            episodes_info.append({\n",
    "                'name': episode_name,\n",
    "                'url': episode_url,\n",
    "                'likes': episode_likes,\n",
    "                'date': episode_date\n",
    "            })\n",
    "\n",
    "    return episodes_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of comments to a dataframe\n",
    "def format_and_save_to_csv(all_comments):\n",
    "    column_names = [\n",
    "        \"Episode_URL\", \"Episode_Name\", \"Episode_Likes\", \"Episode_Date\",\n",
    "        \"Comment_ID\", \"Comment_Reply_Level\", \"Comment_Parent_Comment_No\",\n",
    "        \"Comment_Text\", \"Comment_Date\", \"Comment_Author\",\n",
    "        \"Comment_Likes\", \"Comment_Dislikes\",\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(all_comments, columns=column_names)    \n",
    "\n",
    "    # Convert the Comment_Reply_Level column to integer\n",
    "    df['Comment_Reply_Level'] = df['Comment_Reply_Level'].astype(int)\n",
    "\n",
    "    # max level of reply\n",
    "    max_level = df['Comment_Reply_Level'].max()\n",
    "\n",
    "    if max_level > 1:\n",
    "        # Loop through the levels starting from 2\n",
    "        for level in range(2, max_level + 1):\n",
    "            if level == 2:\n",
    "                # Merge the original DataFrame with the level 2 DataFrame\n",
    "                merged_df = pd.merge(\n",
    "                    df[df['Comment_Reply_Level'] == 1],\n",
    "                    df[df['Comment_Reply_Level'] == 2],\n",
    "                    left_on='Comment_ID',\n",
    "                    right_on='Comment_Parent_Comment_No',\n",
    "                    suffixes=('_level1', '_level2'),\n",
    "                    how='left'\n",
    "                )\n",
    "            else:\n",
    "                # Create a new DataFrame for the current level\n",
    "                df_level = df[df['Comment_Reply_Level'] == level]\n",
    "                \n",
    "                # Merge the current level DataFrame with the previous merged DataFrame\n",
    "                merged_df = pd.merge(\n",
    "                    merged_df,\n",
    "                    df_level,\n",
    "                    left_on='Comment_ID_level{}'.format(level - 1),\n",
    "                    right_on='Comment_Parent_Comment_No_level{}'.format(level),\n",
    "                    suffixes=('_level{}'.format(level - 1), '_level{}'.format(level)),\n",
    "                    how='left'\n",
    "                )\n",
    "\n",
    "        # Select the desired columns for the final DataFrame\n",
    "        final_df = merged_df\n",
    "\n",
    "        # Reset the index of the final DataFrame\n",
    "        final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    else:\n",
    "        # Select the desired columns for the final DataFrame\n",
    "        final_df = df\n",
    "\n",
    "    # Create a new folder with the date and time in the folder name\n",
    "    folder_name = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Use the first 'Episode_URL' to extract genre, series name, episode name, and episode number\n",
    "    first_episode_url = df['Episode_URL'].iloc[-1]\n",
    "    genre, series_name, episode_name, episode_number = parse_url(first_episode_url)\n",
    "\n",
    "    # Save the DataFrame to a CSV file with the date and time in the filename\n",
    "    comments_csv_path = f\"{folder_name}/{genre}_{series_name}_{episode_number}_{episode_name}_comments_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\"\n",
    "    final_df.to_csv(comments_csv_path, index=False, sep=\";\", encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the URL to extract genre, series name, episode name, and episode number\n",
    "def parse_url(url):\n",
    "    # get genre, series name, episode name\n",
    "    path = urlparse(url).path\n",
    "    components = path.split('/')\n",
    "    genre = components[2]\n",
    "    series_name = components[3]\n",
    "    episode_name = components[4]\n",
    "    # get id\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    episode_number = query_params.get('episode_no', [None])[0]\n",
    "    return genre, series_name, episode_name, episode_number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap\n",
    "### Webtoons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrop long : {\"url\": \"https://www.webtoons.com/en/romance/lore-olympus/list?title_no=1320\", \"min_page\": 1, \"max_page\": 25}\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL setup\n",
    "url_list = [\n",
    "    {\"url\": \"https://www.webtoons.com/en/action/weakhero/list?title_no=1726\", \"min_page\": 1, \"max_page\": 24},\n",
    "    {\"url\": \"https://www.webtoons.com/en/challenge/how-to-be-a-dragon/list?title_no=696410\", \"min_page\": 1, \"max_page\": 4},\n",
    "    {\"url\": \"https://www.webtoons.com/en/challenge/nerd-and-jock/list?title_no=135963\", \"min_page\": 1, \"max_page\": 20},\n",
    "    {\"url\": \"https://www.webtoons.com/en/challenge/goth-girl-the-jock/list?title_no=764411\", \"min_page\": 1, \"max_page\": 4},\n",
    "    {\"url\": \"https://www.webtoons.com/en/challenge/seekers-log/list?title_no=102095\", \"min_page\": 1, \"max_page\": 16},\n",
    "    {\"url\": \"https://www.webtoons.com/en/challenge/power-pills/list?title_no=18222\", \"min_page\": 1, \"max_page\": 40},\n",
    "    {\"url\": \"https://www.webtoons.com/en/challenge/a-life-through-selfies/list?title_no=64761\", \"min_page\": 1, \"max_page\": 32},\n",
    "    {\"url\": \"https://www.webtoons.com/en/fantasy/my-husband-changes-every-night/list?title_no=5214\", \"min_page\": 1, \"max_page\": 1},\n",
    "    \n",
    "]\n",
    "\n",
    "'''\n",
    "trop long : {\"url\": \"https://www.webtoons.com/en/romance/lore-olympus/list?title_no=1320\", \"min_page\": 1, \"max_page\": 25}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip episodes already scraped\n",
    "\n",
    "# Directory containing the CSV files\n",
    "csv_directory = \"2023-05-18\"\n",
    "\n",
    "# A list to hold all the data\n",
    "data = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(csv_directory):\n",
    "    # Check if the file is a CSV file\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Construct the full file path\n",
    "        full_filepath = os.path.join(csv_directory, filename)\n",
    "        \n",
    "        # Load the data from the CSV file\n",
    "        df = pl.read_csv(full_filepath, separator=\";\")\n",
    "        \n",
    "        # Add a column to track the original file\n",
    "        df = df.with_columns(pl.lit(filename).alias('original_file'))\n",
    "        \n",
    "        # Add the data to our list\n",
    "        data.append(df)\n",
    "\n",
    "# Combine all the data into one DataFrame\n",
    "all_data = pl.concat(data)\n",
    "\n",
    "# get all the unique values in the first column\n",
    "skip_urls = all_data['Episode_URL_level1'].unique().to_list()\n",
    "\n",
    "# add problematic urls to skip_urls\n",
    "skip_urls.extend(['https://www.webtoons.com/en/action/weakhero/s3-ep-158/viewer?title_no=1726&episode_no=159',\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_webtoons(episodes_info, all_comments, skip_urls):\n",
    "    # Scrape the comments\n",
    "    with tqdm(total=len(episodes_info), desc=\"Scraping comments\") as pbar_outer:\n",
    "        for episode in tqdm(episodes_info, desc=\"Episodes\"):\n",
    "            # skip urls that are already scraped\n",
    "            if episode['url'] not in skip_urls:\n",
    "                comments = scrape_all_comments(driver, [episode])\n",
    "                # Append comments to the DataFrame\n",
    "                all_comments.extend(comments)\n",
    "                # wait a random time between 1 and 5 seconds\n",
    "                time.sleep(random.randint(1, 5))\n",
    "                # Save the comments to a CSV file\n",
    "                format_and_save_to_csv(all_comments)\n",
    "                # Reset all_comments\n",
    "                all_comments = []\n",
    "            # Update the progress bar\n",
    "            pbar_outer.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.81M/6.81M [00:00<00:00, 76.6MB/s]\n",
      "Episodes: 100%|██████████| 240/240 [00:00<00:00, 60015.08it/s]\n",
      "Scraping comments: 100%|██████████| 240/240 [00:00<00:00, 40004.49it/s]\n",
      "Episodes: 100%|██████████| 33/33 [00:00<00:00, 33002.39it/s]\n",
      "Scraping comments: 100%|██████████| 33/33 [00:00<00:00, 8244.21it/s]\n",
      "Scraping comments:   0%|          | 0/199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping comments from Nerd and Jock Ep 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:   0%|          | 0/199 [00:56<?, ?it/s]\n",
      "Scraping comments:   0%|          | 0/199 [00:56<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=113.0.5672.127)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tGetHandleVerifier [0x00388893+48451]\n",
      "\t(No symbol) [0x0031B8A1]\n",
      "\t(No symbol) [0x00225058]\n",
      "\t(No symbol) [0x0020D073]\n",
      "\t(No symbol) [0x0026DEBB]\n",
      "\t(No symbol) [0x0027BFD3]\n",
      "\t(No symbol) [0x0026A0B6]\n",
      "\t(No symbol) [0x00247E08]\n",
      "\t(No symbol) [0x00248F2D]\n",
      "\tGetHandleVerifier [0x005E8E3A+2540266]\n",
      "\tGetHandleVerifier [0x00628959+2801161]\n",
      "\tGetHandleVerifier [0x0062295C+2776588]\n",
      "\tGetHandleVerifier [0x00412280+612144]\n",
      "\t(No symbol) [0x00324F6C]\n",
      "\t(No symbol) [0x003211D8]\n",
      "\t(No symbol) [0x003212BB]\n",
      "\t(No symbol) [0x00314857]\n",
      "\tBaseThreadInitThunk [0x76F400C9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77657B4E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77657B1E+238]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\wem_labo1\\lib\\socket.py:833\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    832\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 833\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m    834\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAn error occurred: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[39m# Close the browser\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     driver\u001b[39m.\u001b[39;49mquit()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\wem_labo1\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:242\u001b[0m, in \u001b[0;36mChromiumDriver.quit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice\u001b[39m.\u001b[39;49mstop()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\wem_labo1\\lib\\site-packages\\selenium\\webdriver\\common\\service.py:145\u001b[0m, in \u001b[0;36mService.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend_remote_shutdown_command()\n\u001b[0;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\wem_labo1\\lib\\site-packages\\selenium\\webdriver\\common\\service.py:129\u001b[0m, in \u001b[0;36mService.send_remote_shutdown_command\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m30\u001b[39m):\n\u001b[1;32m--> 129\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_connectable():\n\u001b[0;32m    130\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     sleep(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\wem_labo1\\lib\\site-packages\\selenium\\webdriver\\common\\service.py:118\u001b[0m, in \u001b[0;36mService.is_connectable\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_connectable\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m    116\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Establishes a socket connection to determine if the service running\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    on the port is accessible.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mis_connectable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\wem_labo1\\lib\\site-packages\\selenium\\webdriver\\common\\utils.py:102\u001b[0m, in \u001b[0;36mis_connectable\u001b[1;34m(port, host)\u001b[0m\n\u001b[0;32m    100\u001b[0m socket_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     socket_ \u001b[39m=\u001b[39m socket\u001b[39m.\u001b[39;49mcreate_connection((host, port), \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    103\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m _is_connectable_exceptions:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\wem_labo1\\lib\\socket.py:833\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[39mif\u001b[39;00m source_address:\n\u001b[0;32m    832\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 833\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m    834\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m    835\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over each URL\n",
    "for url_dict in url_list:\n",
    "    \n",
    "    all_comments = []\n",
    "\n",
    "    # Selenium setup\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--lang=en\") # Set the language to English\n",
    "    #chrome_options.add_argument(\"--headless\") # Run the browser in headless mode, if desired\n",
    "    driver = webdriver.Chrome(service=Service(executable_path=ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        # URL setup\n",
    "        main_url = url_dict[\"url\"]\n",
    "        min_page = url_dict[\"min_page\"]\n",
    "        max_page = url_dict[\"max_page\"]\n",
    "\n",
    "\n",
    "        # Extract the episodes info\n",
    "        episodes_info = extract_episodes_info(driver, main_url, min_page, max_page)\n",
    "\n",
    "        # Scrape the comments\n",
    "        scrape_webtoons(episodes_info, all_comments, skip_urls)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wem_labo1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
